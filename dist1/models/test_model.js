const fs=require("fs"),path=require("path"),tf=require("@tensorflow/tfjs-node"),{registerCustomComponents}=require("./utils/customComponents"),sourceEncoder=require("./utils/sourceEncoder"),sampleTitles=["Scientists discover vaccine that is 100% effective against all diseases","Local community comes together to rebuild after storm damage","Government announces new tax plan starting next month","Sophia Bush Sends Sweet Birthday Message to 'One Tree Hill' Co-Star Hilarie Burton: 'Breyton 4eva'","Is Brad Pitt Open To Getting Back Together With Angelina After Rumors Sheâ€™s Into It?","Sophia Bush Sends Sweet Birthday Message to 'One Tree Hill' Co-Star Hilarie Burton: 'Breyton 4eva'","People's Choice Awards 2018: The best red carpet looks"],sampleSources=["Reuters","Associated Press","CNN","nyfoxnews","etonline","nyfoxnews","today"];function parseWordCounts(e){try{if("object"==typeof e&&e.word_index)return"string"==typeof e.word_index?JSON.parse(e.word_index):e.word_index;if("string"==typeof e&&(e.startsWith('{"<OOV>":')||e.startsWith('{"word_index":')))try{const o=JSON.parse(e);return o.word_index&&"string"==typeof o.word_index?JSON.parse(o.word_index):o.word_index||o}catch(o){const n=e.replace(/^"/,"").replace(/"$/,"").replace(/\\"/g,'"');return JSON.parse(n)}const o=e.replace(/^"/,"").replace(/"$/,"").replace(/\\"/g,'"'),n=JSON.parse(o),t=Object.entries(n).sort(((e,o)=>o[1]-e[1])),s={};let r=1;void 0!==n["<OOV>"]&&(s["<OOV>"]=r++),void 0!==n["<PAD>"]&&(s["<PAD>"]=0);for(const[e,o]of t)"<OOV>"!==e&&"<PAD>"!==e&&(s[e]=r++);return s}catch(o){return console.error("Error parsing word counts:",o),console.error("Input was:",e),{}}}function processText(e,o,n=300,t=2e4){let s=e;(o.config&&o.config.lowercase||o.config&&o.config.lower||o.lowercase)&&(s=s.toLowerCase());const r=o.config&&o.config.filters||o.filters;if(r){const e=new RegExp(`[${r.replace(/[-\/\\^$*+?.()|[\]{}]/g,"\\$&")}]`,"g");s=s.replace(e," ")}const i=o.config&&o.config.split||o.split||" ";let l=[];l=o.config&&o.config.char_level||o.char_level?s.split(""):s.split(i).filter((e=>e.length>0));const c=o.word_index||{},a=o.oov_token_id||(o.config&&o.config.oov_token?c[o.config.oov_token]:1),d=Math.min(a,t-1),g=[],u=[];l.forEach((e=>{let o=c[e]||d;(o>=t||o<0)&&(console.warn(`Token "${e}" mapped to invalid ID ${o}, using OOV token ID ${d} instead`),o=d),g.push(o),u.push({token:e,id:o,isOOV:void 0===c[e],clamped:(c[e]||d)!==o})}));const p=[...u];if(g.length>n)return{sequence:g.slice(0,n),tokenDebug:u.slice(0,n),originalDebug:p,truncated:!0};const f=o.pad_token_id||0;for(;g.length<n;)g.push(f),u.push({token:"<PAD>",id:f,isPadding:!0});return{sequence:g,tokenDebug:u,originalDebug:p,truncated:!1,padded:g.length>p.length}}function validateSequence(e,o){const n=[];for(let t=0;t<e.length;t++)(e[t]>=o||e[t]<0)&&n.push({position:t,value:e[t]});return{valid:0===n.length,outOfRangeIndices:n,maxIndex:Math.max(...e),minIndex:Math.min(...e)}}function mapSourceToId(e){if(sourceEncoder.initialized){const o=sourceEncoder.getSourceIndex(e);return o.index<0&&(console.warn(`Warning: Negative source index ${o.index} detected for "${e}", using fallback index 1`),o.index=1),o}return{index:Math.abs(e.split("").reduce(((e,o)=>e+o.charCodeAt(0)),0))%2030+1,type:"hash",found:!1,sourceName:e,reason:"Source encoder not initialized"}}function loadTokenizer(e){try{const o=e.replace(".json","_converted.json");if(fs.existsSync(o)){console.log(`Using converted tokenizer from ${o}`);const e=fs.readFileSync(o,"utf8"),n=JSON.parse(e);return n.word_index&&"string"==typeof n.word_index&&(n.word_index=JSON.parse(n.word_index)),n}console.log(`Loading original tokenizer from ${e}`);const n=fs.readFileSync(e,"utf8"),t=JSON.parse(n);return t.word_index&&"string"==typeof t.word_index&&(t.word_index=JSON.parse(t.word_index)),t}catch(o){return console.error(`Error loading tokenizer from ${e}:`,o),null}}async function testModel(){try{console.log("Starting model testing..."),registerCustomComponents();const e="file://./fake_news_model/model.json",o="./fake_news_model/tokenizer.json",n="./fake_news_model/source_encoder.json";sourceEncoder.initialized||sourceEncoder.loadFromFile(n),console.log("Loading tokenizer...");const t=loadTokenizer(o);t?console.log("Tokenizer loaded successfully"):console.error("Failed to load tokenizer. Using fallback tokenization.");const s=t?.text_tokenizer||t;console.log("Loading model...");const r=await tf.loadLayersModel(e);console.log("Model loaded successfully");const i=r.layers.find((e=>"text_embedding"===e.name)),l=i?i.getConfig().inputDim:2e4,c=r.layers.find((e=>"source_embedding"===e.name)),a=c?c.getConfig().inputDim:2031;console.log(`Model text vocabulary size: ${l}`),console.log(`Model source vocabulary size: ${a}`),console.log("Model input shapes:",r.inputs.map((e=>`${e.name}: [${e.shape}]`))),console.log("Model output shape:",r.outputs[0].shape),console.log("\n=== INDIVIDUAL NEWS ARTICLES ===");for(let e=0;e<sampleTitles.length;e++){const o=sampleTitles[e],n=sampleSources[e%sampleSources.length];await testSingleExample(r,o,n,s)}console.log("\n=== SOURCE INFLUENCE TEST ===");const d="Breaking: Scientists discover breakthrough treatment";console.log(`Testing headline "${d}" across different sources:`);for(const e of sampleSources)await testSingleExample(r,d,e,s,!0)}catch(e){console.error("Error during model testing:",e)}}async function testSingleExample(e,o,n,t,s=!1){try{const r=e.layers&&e.layers.find((e=>"text_embedding"===e.name))?.getConfig().inputDim||2e4,i=e.layers&&e.layers.find((e=>"source_embedding"===e.name))?.getConfig().inputDim||2031,l=processText(o,t,300,r),c=mapSourceToId(n),a=validateSequence(l.sequence,r);a.valid||(console.warn(`Warning: ${a.outOfRangeIndices.length} tokens are outside valid range [0,${r-1}]`),console.warn(`Range in sequence: [${a.minIndex},${a.maxIndex}]`));const d=tf.tensor2d([l.sequence],[1,300]),g=tf.tensor2d([[Math.min(c.index,i-1)]],[1,1]),u=e.predict([d,g]),p=u.dataSync()[0];if(s)console.log(`Source: ${n.padEnd(20)} | ID: ${c.index} | Score: ${p.toFixed(4)} | ${p>=.5?" LIKELY REAL":" LIKELY Fake"}`);else{console.log("\n--------------------------------------"),console.log("Title:",o),console.log("Source:",n),console.log("\nTOKENIZATION DETAILS:"),console.log(`Original text tokens (${l.originalDebug.length}):`);const e=l.originalDebug.map((e=>`${e.token}(${e.id}${e.clamped?"*":""})`)).join(", ");console.log(e.length>100?e.substring(0,100)+"...":e),l.truncated&&console.log(`Note: Text was truncated from ${l.originalDebug.length} to 300 tokens`),l.padded&&console.log(`Note: Text was padded with ${l.sequence.length-l.originalDebug.length} padding tokens`),console.log("\nSOURCE DETAILS:");const t=Math.min(c.index,i-1);console.log(`Source "${n}" mapped to ID: ${t}${t!==c.index?` (clamped from ${c.index})`:""}`),c.found||("hash"===c.type?console.log("Source ID was generated with hash function (not found in vocabulary)"):(console.log("Source was not found in source encoder"),c.suggestions&&c.suggestions.length>0&&(console.log("Similar sources:"),c.suggestions.forEach((e=>{console.log(`- "${e}"`)}))))),console.log("\nMODEL PREDICTION:"),console.log("Fake news probability:",p.toFixed(4)),console.log("Assessment:",p>=.5?"LIKELY REAL":"LIKELY FAKE"),console.log("--------------------------------------")}tf.dispose([d,g,u])}catch(e){console.error(`Error testing example "${o}" from "${n}":`,e)}}testModel().then((()=>{console.log("\nModel testing completed")})).catch((e=>{console.error("Error during testing:",e)})).finally((()=>{setTimeout((()=>process.exit()),1e3)}));