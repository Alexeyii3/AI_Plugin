class ModelLoader{constructor(){this.model=null,this.tokenizer=null,this.isLoading=!1}isTensorFlowAvailable(){return void 0!==window.tf}async loadModel(e){if(this.model)return this.model;if(this.isLoading){for(;this.isLoading;)await new Promise((e=>setTimeout(e,100)));return this.model}this.isLoading=!0;try{if(!this.isTensorFlowAvailable())throw new Error("TensorFlow.js is not available. Make sure it was properly injected by the extension.");const o=chrome.runtime.getURL(e);return console.log("Loading model from:",o),this.model=await window.tf.loadLayersModel(o),console.log("Model loaded successfully"),this.isLoading=!1,this.model}catch(e){throw console.error("Error loading model:",e),this.isLoading=!1,e}}async loadTokenizer(e){if(this.tokenizer)return this.tokenizer;try{const o=chrome.runtime.getURL(e);console.log("Loading tokenizer from:",o);const t=await fetch(o);if(!t.ok)throw new Error(`Failed to fetch tokenizer: ${t.status}`);const r=await t.json();return this.tokenizer={word_index:r,oov_token:"<UNK>",max_len:30},console.log("Tokenizer loaded successfully"),this.tokenizer}catch(e){throw console.error("Error loading tokenizer:",e),e}}textToSequence(e,o){if(!this.tokenizer||!this.tokenizer.word_index)return console.error("Tokenizer not loaded"),Array(o||30).fill(0);const t=e.toLowerCase().replace(/<[^>]+>/g,"").replace(/[^\w\s]/g," ").replace(/\s+/g," ").trim().split(" "),r=this.tokenizer.word_index["<UNK>"]||1,i=t.map((e=>this.tokenizer.word_index[e]||r)),n=o||this.tokenizer.max_len||30;return i.length>n?i.slice(0,n):[...i,...Array(n-i.length).fill(0)]}async predict(e){if(!this.model||!this.tokenizer)throw new Error("Model or tokenizer not loaded");try{const o=this.textToSequence(e),t=window.tf.tensor2d([o],[1,o.length]),r=this.model.predict(t),i=r.dataSync()[0];return window.tf.dispose([t,r]),{probability:i,isClickbait:i>.5}}catch(e){throw console.error("Prediction error:",e),e}}}const modelLoader=new ModelLoader;